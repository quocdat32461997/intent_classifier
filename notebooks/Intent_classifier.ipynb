{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intent Classification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clinc 150 Dataset from UCI\n",
    "[An evaluation datset for intent classification and out-of-scope prediction.](https://archive.ics.uci.edu/ml/datasets/CLINC150)\n",
    "\n",
    "**Citation**\n",
    "* Stefan Larson, Anish Mahendran, Joseph J. Peper, Christopher Clarke, Andrew Lee, Parker Hill, Jonathan K. Kummerfeld, Kevin Leach, Michael A. Laurenzano, Lingjia Tang, and Jason Mars. 2019. An evaluation dataset for intent classification and out-of-scope prediction. In Proceedings of EMNLP-IJCNLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../data/clinc150_uci/data_full.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path) as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['oos_val', 'val', 'train', 'oos_test', 'test', 'oos_train'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### collect texts and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = []\n",
    "train_labels = []\n",
    "val_texts = []\n",
    "val_labels = []\n",
    "test_texts = []\n",
    "test_labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect texts and labels in train and oos_train set\n",
    "for item in data['train']:\n",
    "    train_texts.append(item[0])\n",
    "    train_labels.append(item[-1])\n",
    "# collect texts and labels in oos-train set\n",
    "for item in data['oos_train']:\n",
    "    train_texts.append(item[0])\n",
    "    train_labels.append(item[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect texts and labels in val and oos_val set\n",
    "for item in data['val']:\n",
    "    val_texts.append(item[0])\n",
    "    val_labels.append(item[-1])\n",
    "# collect texts and labels in oos-train set\n",
    "for item in data['oos_val']:\n",
    "    val_texts.append(item[0])\n",
    "    val_labels.append(item[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect texts and labels in test and oos_test set\n",
    "for item in data['test']:\n",
    "    test_texts.append(item[0])\n",
    "    test_labels.append(item[-1])\n",
    "# collect texts and labels in oos-train set\n",
    "for item in data['oos_test']:\n",
    "    test_texts.append(item[0])\n",
    "    test_labels.append(item[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['play_music',\n",
       " 'next_song',\n",
       " 'application_status',\n",
       " 'plug_type',\n",
       " 'gas',\n",
       " 'order',\n",
       " 'credit_limit_change',\n",
       " 'directions',\n",
       " 'meaning_of_life',\n",
       " 'greeting',\n",
       " 'calculator',\n",
       " 'change_ai_name',\n",
       " 'insurance',\n",
       " 'reset_settings',\n",
       " 'weather',\n",
       " 'oil_change_how',\n",
       " 'date',\n",
       " 'pto_balance',\n",
       " 'reminder',\n",
       " 'todo_list',\n",
       " 'order_checks',\n",
       " 'yes',\n",
       " 'do_you_have_pets',\n",
       " 'spending_history',\n",
       " 'what_can_i_ask_you',\n",
       " 'maybe',\n",
       " 'rewards_balance',\n",
       " 'schedule_meeting',\n",
       " 'recipe',\n",
       " 'goodbye',\n",
       " 'cancel',\n",
       " 'mpg',\n",
       " 'report_lost_card',\n",
       " 'restaurant_suggestion',\n",
       " 'tell_joke',\n",
       " 'bill_due',\n",
       " 'book_flight',\n",
       " 'ingredients_list',\n",
       " 'change_accent',\n",
       " 'interest_rate',\n",
       " 'book_hotel',\n",
       " 'last_maintenance',\n",
       " 'what_is_your_name',\n",
       " 'international_visa',\n",
       " 'timer',\n",
       " 'expiration_date',\n",
       " 'direct_deposit',\n",
       " 'car_rental',\n",
       " 'card_declined',\n",
       " 'pin_change',\n",
       " 'account_blocked',\n",
       " 'user_name',\n",
       " 'definition',\n",
       " 'tire_pressure',\n",
       " 'calendar_update',\n",
       " 'smart_home',\n",
       " 'income',\n",
       " 'change_language',\n",
       " 'cook_time',\n",
       " 'accept_reservations',\n",
       " 'restaurant_reviews',\n",
       " 'meal_suggestion',\n",
       " 'w2',\n",
       " 'no',\n",
       " 'alarm',\n",
       " 'what_song',\n",
       " 'todo_list_update',\n",
       " 'transfer',\n",
       " 'thank_you',\n",
       " 'international_fees',\n",
       " 'measurement_conversion',\n",
       " 'freeze_account',\n",
       " 'spelling',\n",
       " 'new_card',\n",
       " 'transactions',\n",
       " 'pto_used',\n",
       " 'text',\n",
       " 'tire_change',\n",
       " 'find_phone',\n",
       " 'balance',\n",
       " 'replacement_card_duration',\n",
       " 'change_volume',\n",
       " 'where_are_you_from',\n",
       " 'food_last',\n",
       " 'ingredient_substitution',\n",
       " 'next_holiday',\n",
       " 'change_speed',\n",
       " 'what_are_your_hobbies',\n",
       " 'exchange_rate',\n",
       " 'timezone',\n",
       " 'apr',\n",
       " 'translate',\n",
       " 'uber',\n",
       " 'reminder_update',\n",
       " 'traffic',\n",
       " 'travel_notification',\n",
       " 'damaged_card',\n",
       " 'bill_balance',\n",
       " 'flight_status',\n",
       " 'whisper_mode',\n",
       " 'gas_type',\n",
       " 'fun_fact',\n",
       " 'improve_credit_score',\n",
       " 'routing',\n",
       " 'time',\n",
       " 'travel_alert',\n",
       " 'pay_bill',\n",
       " 'taxes',\n",
       " 'meeting_schedule',\n",
       " 'order_status',\n",
       " 'payday',\n",
       " 'sync_device',\n",
       " 'oos',\n",
       " 'schedule_maintenance',\n",
       " 'lost_luggage',\n",
       " 'update_playlist',\n",
       " 'calories',\n",
       " 'distance',\n",
       " 'roll_dice',\n",
       " 'nutrition_info',\n",
       " 'change_user_name',\n",
       " 'cancel_reservation',\n",
       " 'pto_request',\n",
       " 'jump_start',\n",
       " 'travel_suggestion',\n",
       " 'share_location',\n",
       " 'are_you_a_bot',\n",
       " 'rollover_401k',\n",
       " 'redeem_rewards',\n",
       " 'shopping_list_update',\n",
       " 'who_made_you',\n",
       " 'who_do_you_work_for',\n",
       " 'min_payment',\n",
       " 'credit_limit',\n",
       " 'how_busy',\n",
       " 'report_fraud',\n",
       " 'carry_on',\n",
       " 'insurance_change',\n",
       " 'pto_request_status',\n",
       " 'how_old_are_you',\n",
       " 'confirm_reservation',\n",
       " 'shopping_list',\n",
       " 'repeat',\n",
       " 'flip_coin',\n",
       " 'calendar',\n",
       " 'restaurant_reservation',\n",
       " 'oil_change_when',\n",
       " 'vaccines',\n",
       " 'current_location',\n",
       " 'credit_score',\n",
       " 'make_call']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# collect unique labels\n",
    "label_set = list(set(train_labels + val_labels + test_labels))\n",
    "label_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(label_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode categories\n",
    "\n",
    "# initializer label encoder\n",
    "le = LabelEncoder()\n",
    "le.fit(label_set)\n",
    "\n",
    "# encode labels\n",
    "train_y = le.transform(train_labels)\n",
    "val_y = le.transform(val_labels)\n",
    "test_y = le.transform(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Text Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(tokenizer = tokenizer.tokenize, analyzer = 'word', stop_words = 'english').fit(train_texts + val_texts + test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = tfidf_vectorizer.transform(train_texts)\n",
    "val_X = tfidf_vectorizer.transform(val_texts)\n",
    "test_X = tfidf_vectorizer.transform(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15100, 7022)\n"
     ]
    }
   ],
   "source": [
    "print(train_X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializer SVM classifier\n",
    "svm = SVC(C = 2, class_weight = 'balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=2, class_weight='balanced')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training SVM\n",
    "svm.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9794701986754967"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm.score(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.817741935483871"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm.score(val_X, val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8016363636363636"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm.score(test_X, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AdaBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializer Gradient Boosting Classifier\n",
    "gb_classifier = GradientBoostingClassifier(learning_rate = 0.01, n_iter_no_change = 10, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           3.9994           12.71m\n",
      "         2           3.7270           13.15m\n",
      "         3           3.5334           13.10m\n",
      "         4           3.3805           12.90m\n",
      "         5           3.2546           12.78m\n",
      "         6           3.1468           12.69m\n",
      "         7           3.0531           12.53m\n",
      "         8           2.9653           12.36m\n",
      "         9           2.8891           12.20m\n",
      "        10           2.8169           12.04m\n",
      "        20           2.3170           10.60m\n",
      "        30           2.0055            9.24m\n",
      "        40           1.7720            7.91m\n",
      "        50           1.5912            6.60m\n",
      "        60           1.4391            5.27m\n",
      "        70           1.3104            3.95m\n",
      "        80           1.1980            2.63m\n",
      "        90           1.1021            1.31m\n",
      "       100           1.0179            0.00s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(learning_rate=0.01, n_iter_no_change=10, verbose=1)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train gb_classifier\n",
    "gb_classifier.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7316129032258064"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb_classifier.score(val_X, val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6163636363636363"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb_classifier.score(test_X, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model and intent list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "filename = '../intent_classifier.sav'\n",
    "pickle.dump(svm, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append new_line character\n",
    "labels = [label+ '\\n' for label in label_set]\n",
    "labels[-1] = labels[-1].strip('\\n')\n",
    "\n",
    "# save intent list\n",
    "with open('../intent_list.txt', 'w') as file:\n",
    "    file.writelines(labels)\n",
    "    \n",
    "# save vocabs\n",
    "with open('../vocabs.pickle', 'wb') as file:\n",
    "    vocabs = tfidf_vectorizer.vocabulary_\n",
    "    pickle.dump(vocabs, file)\n",
    "    \n",
    "# save vectorizer\n",
    "with open('../tfidf_vectorizer.pickle', 'wb') as file:\n",
    "    pickle.dump(tfidf_vectorizer, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
